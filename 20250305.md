# Information
- **Time:** 13:30 - 
- **Attendees:** Bob Zhang (Supervisor), Huang Yanzhen, Mai Jiajun
# Discussion Summary

## âœ¨ What we've done
- We applied per-channel normalization for data samples.
- We added a batch-normalization layer after all the convolution layers (before activation).
- We replaced the original MaxPool3D layer with our custom ResPool layer, which added the batch-normed data back to the original input  (so we named it the Residual Pooling layer). Unlike max-polling in 3D that loses 87.5% of the original information, ResPool only emphasized the local max from the original, boosting the model's learning performance.
- We removed the 4th convolution layer (C=32 -> C=32) and added a fully-connected layer. This prevents over-fitting.

## ðŸ‘ What's improved
- Training: 200 epoch - Min validation loss = 19.0367 => 25 epoch - Min validation loss = 11.0318
- Application: No more after-detection bias needed.

## ðŸ‘€ Performance

### Training Performance

<div style="display: flex; flex-wrap: wrap;">
<img src="https://github.com/user-attachments/assets/f69c45eb-c3e6-4676-b6a6-a4a112e220e7" height="250"/>
<img src="https://github.com/user-attachments/assets/998efb21-537b-43f0-b1c1-a7d8880427e7" height="250"/>
<img src="https://github.com/user-attachments/assets/d67606ed-a906-42bf-8fa8-418cd5cf5d1f" height="250"/>
<img src="https://github.com/user-attachments/assets/7bc5be23-a060-49bf-b011-d6e79efaa02d" height="250"/>
</div>

### Application Performance - Average computation time per frame (seconds)

<img src="https://github.com/user-attachments/assets/34e2e8b8-0726-4ebc-8872-d687931f5ba2" height="250"/>


