# Information
- **Time:** 13:20 - 13:45 
- **Attendees:** Bob Zhang (Supervisor), Huang Yanzhen, Mai Jiajun
# Discussion Summary

## 1. Revision of Step 02: Posture Recognition Model Training
### 1.1 Network Structure
- We applied per-channel normalization for data samples.
- We added a batch-normalization layer after all the convolution layers (before activation).
- We replaced the original MaxPool3D layer with our custom ResPool layer, which added the batch-normed data back to the original input  (so we named it the Residual Pooling layer). Unlike max-polling in 3D that loses 87.5% of the original information, ResPool only emphasized the local max from the original, boosting the model's learning performance.
- We removed the 4th convolution layer (C=32 -> C=32) and added a fully-connected layer. This prevents over-fitting.

### 1.2 Combined Loss
- To further improve the model's generalization ability, We applied the method of combining 1 main criterion and 2 associate criterions together as a WeightedLoss we used in classifier training.
	- The main criterion is CrossEntropyLoss.
	- The other 2 criterion is FocalLoss and L2_regularization.
	- The weights of criterions are 0.6, 0.3, and 0.1.
$$\begin{align}
\mathcal{L}_\text{total}=\ &w_1\cdot\mathcal{L}_\text{CE}+
w_2\cdot\mathcal{L}_\text{Focal }+w_3\cdot\mathcal{L}_\text{L2} \\
=\ &w_1\cdot [-\ln(p_t)]\ + w_2\cdot \alpha\cdot(1-p_t)^\gamma\cdot(-\ln(p_t))\ + w_3\cdot\lambda_\text{L2}\times\sum\|\text{param}\|_2\\
\end{align}$$

### 1.3 Performance
 Training: Early stopped at 26th epoch
```sh
Epoch[026/650], Train Loss:7.5009, Valid Loss:12.1534, OFF: 0.7858 | Cur Optim: 1967384384992, Min VL: 10.0571, Num OF epochs: 4
```

Application: No more after-detection bias needed.

<div style="display: flex; flex-wrap: wrap;">
<img src="https://github.com/user-attachments/assets/be1cc700-0aa5-4675-a3fa-4ef7aff4a581" height="250"/>
<img src="https://github.com/user-attachments/assets/8e1abd5f-e2b8-408c-8478-508146f5ba91" height="250"/>
<img src="https://github.com/user-attachments/assets/40f7466d-91c0-40cf-92f4-c069762a7419" height="250"/>
<img src="https://github.com/user-attachments/assets/aa1c0a7f-a2d9-4a19-9b9f-2253b9ed3ed2" height="250"/>
</div>

## 2. Face Recognition Backend
- Use `dlib` to achieve similar task as RTMPose:
	- Use `dlib`'s pre-trained face detector to detect faces and select the first face in the given image.
	- Use `dlib`'s pre-trained shape predictor to extract 68 face landmarks of the given image.
	- Use `dlib`'s pre-trained face recognition model to convert face landmarks to a 128-length feature vector.
	- *(We didn't train or fine-tune any of the `dlib`'s model. PyTorch is not required by `dlib`.)*
- To compare faces, we calculate the inversed Euclidean distance between the sample face vector and target face vectors.
- Face features are stored along with the face base64 string in PostgreSQL database to save query performance.

Storage of faces in the database:

<img src="https://s2.loli.net/2025/03/07/sKCoRnjVXpOwfeS.png" >

### 2.1 Performance on Siblings
Comparing a sample face of Small S (Dee Hsu) among all the faces (including male & female). The most similar face to the sample is the one of Big S (Barbie Hsu), Small S's elder sister. However, since they are two different people after all, the top rank's score is not significantly high compared to others.

<img src="https://s2.loli.net/2025/03/07/rUlhsyS68XYdAnH.png" height="400px">

### 2.2 Performance on Exact Same Person
In this test, two images of different ages of the same person is used. Here, since they are the same person, the top rank's score is significantly higher than the others. Also, thanks to a preliminary caching policy, the query time at this stage is reduced.

<img src="https://s2.loli.net/2025/03/07/WgtqwR9MZO7aGfE.png" height="400px">

# Next Meeting's Agenda
- Caching policy of the database: A more flexible way of caching query results, including the usage of score thresholds, result paging, etc.